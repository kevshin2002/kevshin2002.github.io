(self.webpackChunkkevshin2002=self.webpackChunkkevshin2002||[]).push([[6782],{9020:(e,n,t)=>{e.exports={src:{srcSet:t.p+"assets/ideal-img/cv_bridge.00e954b.500.png 500w,"+t.p+"assets/ideal-img/cv_bridge.6472653.1000.png 1000w,"+t.p+"assets/ideal-img/cv_bridge.448c97e.1021.png 1021w",images:[{path:t.p+"assets/ideal-img/cv_bridge.00e954b.500.png",width:500,height:352},{path:t.p+"assets/ideal-img/cv_bridge.6472653.1000.png",width:1e3,height:704},{path:t.p+"assets/ideal-img/cv_bridge.448c97e.1021.png",width:1021,height:719}],src:t.p+"assets/ideal-img/cv_bridge.00e954b.500.png",toString:function(){return t.p+"assets/ideal-img/cv_bridge.00e954b.500.png"},placeholder:void 0,width:500,height:352},preSrc:"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAHCAYAAAAxrNxjAAAACXBIWXMAAAsTAAALEwEAmpwYAAAA/klEQVR4nC3Oy0vCAADH8f3FXjoFdrFDRMxO1alUKlZQ4YsSoTBfTEWdZTnfvXS6DfIF6dR9I+0Hn+uPr6ANpjQeCzTzLyhNA6XSpahqKP+qbQPbthGyzQ82vPvs7sj4U1W2DwM4XCc4XB5Eb5Rwoo5lLRB65pBQJs+D3Oa1paOoXdL5GtmnNoraofZmrh9nswXDgcVg9INuTla+tG96xpi+MWE0nvI3oVzvcxrKcRUtEY6rHF0mcYoSW+4LfMEc95kW8/kSoVjROJBi+PwywXgV0RNhc+8Mp1vi+CbNXaqxbuz0hyQL78ilz5XrSA4pmObcn+A29ky5rrNc2vwCaTTuWbVd5OAAAAAASUVORK5CYII="}},3030:(e,n,t)=>{e.exports={src:{srcSet:t.p+"assets/ideal-img/hsv.f48adf7.500.jpg 500w,"+t.p+"assets/ideal-img/hsv.2d5b67a.630.jpg 630w",images:[{path:t.p+"assets/ideal-img/hsv.f48adf7.500.jpg",width:500,height:267},{path:t.p+"assets/ideal-img/hsv.2d5b67a.630.jpg",width:630,height:337}],src:t.p+"assets/ideal-img/hsv.f48adf7.500.jpg",toString:function(){return t.p+"assets/ideal-img/hsv.f48adf7.500.jpg"},placeholder:void 0,width:500,height:267},preSrc:"data:image/jpeg;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAFCAIAAADzBuo/AAAACXBIWXMAAAsTAAALEwEAmpwYAAAAi0lEQVR4nGNwdHR0cXF2dHT09/ePiY3x8/cTFBblFxTlFxRh5+JhQAPMTEypGbmbt+89eOTkslXrGJhhgJGJhZGZhYmZadHCBW/fv3//9efpcxcZGGH6JJgYhBhBvKlTply8ev3ouStrNm1jUFdTVVHXsNbXvhGu/SRON8VCnYGVXVRYSFxURFhQEADFeiZ94r7kjgAAAABJRU5ErkJggg=="}},2956:(e,n,t)=>{e.exports={src:{srcSet:t.p+"assets/ideal-img/pc_array.6c48c57.500.png 500w,"+t.p+"assets/ideal-img/pc_array.df24571.1000.png 1000w,"+t.p+"assets/ideal-img/pc_array.0740561.1094.png 1094w",images:[{path:t.p+"assets/ideal-img/pc_array.6c48c57.500.png",width:500,height:274},{path:t.p+"assets/ideal-img/pc_array.df24571.1000.png",width:1e3,height:548},{path:t.p+"assets/ideal-img/pc_array.0740561.1094.png",width:1094,height:599}],src:t.p+"assets/ideal-img/pc_array.6c48c57.500.png",toString:function(){return t.p+"assets/ideal-img/pc_array.6c48c57.500.png"},placeholder:void 0,width:500,height:274},preSrc:"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAFCAYAAAB8ZH1oAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAiklEQVR4nE2OSw6CMAAFe//baVwQ/CClUErE6qKlVtoxkECc1cts5olpmmjblqZpUErhvSfGiJQSay0LOWdEmr/r2MQ8z+ScSCntfkEcyxYjS2R9oFQXxsfItbijux5jDFprhmFAnM4dVt/o+4JqqPHOUxWK1/NNCBPO+fWOCCHgnOMT4575T27pH3Dsv3zcybOEAAAAAElFTkSuQmCC"}},7717:(e,n,t)=>{"use strict";t.r(n),t.d(n,{assets:()=>x,contentTitle:()=>b,default:()=>y,frontMatter:()=>f,metadata:()=>s,toc:()=>_});const s=JSON.parse('{"id":"Robotics/ROS2/perception","title":"Perception","description":"This has not been proof-read or modified for simpler reading yet.","source":"@site/docs/Robotics/ROS2/perception.mdx","sourceDirName":"Robotics/ROS2","slug":"/Robotics/ROS2/perception","permalink":"/docs/Robotics/ROS2/perception","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1704614400000,"sidebarPosition":2,"frontMatter":{"id":"perception","title":"Perception","hide_title":true,"sidebar_position":2,"last_update":{"date":"1/7/2024","author":"Kevin Shin"}},"sidebar":"noteSidebar","previous":{"title":"Nav2","permalink":"/docs/Robotics/ROS2/nav2"},"next":{"title":"Testing","permalink":"/docs/Robotics/ROS2/testing-ros2"}}');var r=t(4848),i=t(8453),o=t(1470),a=t(9365);const l=t.p+"assets/images/pixels-deea6f0211d05ded36309eb8dafe0bcf.gif";var c=t(2956),d=t.n(c);const h=t.p+"assets/images/pc_marker_node-c750317a56f4a5fd7870a24a65cd23fd.gif";var p=t(9020),u=t.n(p),m=t(3030),g=t.n(m);const f={id:"perception",title:"Perception",hide_title:!0,sidebar_position:2,last_update:{date:"1/7/2024",author:"Kevin Shin"}},b=void 0,x={},_=[{value:"Getting Started",id:"getting-started",level:2},{value:"Sensor Data",id:"sensor-data",level:2},{value:"Laser Messages",id:"laser-messages",level:3},{value:"Image Messages",id:"image-messages",level:3},{value:"Point Cloud Messages",id:"point-cloud-messages",level:3},{value:"Point Cloud Marker written by The Construct",id:"point-cloud-marker-written-by-the-construct",level:5},{value:"Image Processing",id:"image-processing",level:2},{value:"OpenCV",id:"opencv",level:3},{value:"cv_bridge",id:"cv_bridge",level:3},{value:"Color Spaces",id:"color-spaces",level:3},{value:"RGB vs HSV",id:"rgb-vs-hsv",level:4},{value:"Advantages of HSV over RGB for Image Processing",id:"advantages-of-hsv-over-rgb-for-image-processing",level:4},{value:"Blob Tracking",id:"blob-tracking",level:2},{value:"Blob Tracker Node",id:"blob-tracker-node",level:3}];function v(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",h4:"h4",h5:"h5",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsxs)(n.admonition,{type:"danger",children:[(0,r.jsx)(n.p,{children:"This has not been proof-read or modified for simpler reading yet."}),(0,r.jsx)(n.p,{children:"12/23/24"})]}),"\n",(0,r.jsx)(n.h2,{id:"getting-started",children:"Getting Started"}),"\n",(0,r.jsxs)(n.p,{children:["These are my notes from the ROS2 Perception course available on ",(0,r.jsx)(n.a,{href:"https://app.theconstruct.ai",children:"The Construct"}),". Since this is a robotics note, you will need a physical robot to follow along. I am using the simulator on the website to interact with the tech stack."]}),"\n",(0,r.jsx)(n.p,{children:"This note will cover:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Integrating and processing sensor data in ROS2"}),"\n",(0,r.jsx)(n.li,{children:"Implementing image processing algorithms using OpenCV"}),"\n",(0,r.jsx)(n.li,{children:"Working with Point Cloud Data and Point Cloud Library (PCL)"}),"\n",(0,r.jsx)(n.li,{children:"Exploring human-robot interaction with face detection and recognition"}),"\n",(0,r.jsx)(n.li,{children:"Applying AI perception techniques using deep learning frameworks"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"sensor-data",children:"Sensor Data"}),"\n",(0,r.jsx)(n.p,{children:"Sensors make up robotics, and it's important to understand how they work and how to work with them. As robots have various sensors, our goal is to utilize them in order to apply perception and decision making algorithms. Therefore, combining information from multiple sensors such as cameras, LIDAR, radar, and other specialized devices can help us construct a more detailed and accurate representation of the environment that'll aid us in our algorithms."}),"\n",(0,r.jsx)(n.p,{children:"In ROS2, there are different kinds of messages that help robots understand the world. The three main messages though are:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"sensor_msgs/LaserScan"})," - Maps out surroundings using LIDAR which gives robot the ability to localize themselves and avoid collisions"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"sensor_msgs/Image"})," - Captures images with a camera and helps in recognizing objects via computer vision."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"sensor_msgs/PointCloud2"})," - Captures detailed information about shapes and structures in the environment to create 3D maps."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"laser-messages",children:"Laser Messages"}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.code,{children:"sensor_msgs/LaserScan"})," is a standard message type commonly used for representing laser scan data generated by laser range finders or LIDAR sensors. Laser scan data represents the distance measurements captured by a LIDAR. More information can be found on the ",(0,r.jsx)(n.a,{href:"https://docs.ros.org/en/noetic/api/sensor_msgs/html/msg/LaserScan.html",children:"official documentation"})]}),"\n",(0,r.jsx)(n.p,{children:"There are other fields but the most important ones are:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"angle_min"})," and ",(0,r.jsx)(n.code,{children:"angle_max"})," - represents minimum and maximum angles of the laser scan, which defines the angular range over which the laser sensor collected data"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"angle_increment"})," - specifies the angular distance between consecutive measurements in the scan and helps to determine the angular resolution of the sensor."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"range_min"})," and ",(0,r.jsx)(n.code,{children:"range_max"})," - defines the minimum and maximum range of the sensor"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"range"})," - an array containing the measured distances from the sensor to objects in its field of view where each element corresponds to a specific angle within the angular ranged defined by the ",(0,r.jsx)(n.code,{children:"angle_min"}),", ",(0,r.jsx)(n.code,{children:"angle_max"}),", and ",(0,r.jsx)(n.code,{children:"angle_increment"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"intensities"})," - an array that provides intensity information along with range data. (THis could be empty as not all laser scanners provide intensity data)"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"image-messages",children:"Image Messages"}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.code,{children:"sensor_msgs/Image"})," is another standard message type used for representing images. Images captured by sensors such as cameras are represented as a grid of pixels where each pixel contains information about its color and intensity. The grid is a compactly encoded one dimensional array that's stored in the data field. More can be found ",(0,r.jsx)(n.a,{href:"https://docs.ros.org/en/noetic/api/sensor_msgs/html/msg/Image.html",children:"here"})]}),"\n",(0,r.jsx)(n.p,{children:"The most important ones are:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"height"})," and ",(0,r.jsx)(n.code,{children:"width"})," - represents the dimensions of the image. Height = rows, width = columns"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"encoding"})," - represents the format used to represent the image data. Commmon encodings are below:"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"bgr8"})," - 8-bit BGR (Blue-Green-Red) encoding where each pixel is represented by three bytes"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"rgb8"})," - 8-bit RGB (Red-Green-Blue) encoding similar to ",(0,r.jsx)(n.code,{children:"bgr8"})," but with a different byte order"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"mono8"})," - 8-bit grayscale encoding, wehre each pixel is represented by a single byte indicating its intensity"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"step"})," - specifies the number of bytes occupied by each row of the image data (row length) in bytes. The formula would thus be byte_depth * row length (width) = step."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"data"})," - contains the raw image data and is represented as an array of bytes, not pixels where each byte represents a component of the image data according to the specified encoding."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.admonition,{type:"note",children:(0,r.jsx)(n.p,{children:"Images generally use the top left corner of an image as the origin (0.0). This aligns with computer graphic standards and is the usual convention. Also note that the values of each color component ranges from 0 to 255, with 0 being no color and 255 representing maximum intensity."})}),"\n",(0,r.jsx)("img",{src:l,width:"100%"}),"\n",(0,r.jsxs)(n.p,{children:["The data for the above grid would be: ",(0,r.jsx)(n.code,{children:"data"})," = ",(0,r.jsx)(n.code,{children:"[ B1, G1, R1, B2, G2, R2, B3, G3, R3, B4, G4, R4]"})," since it uses ",(0,r.jsx)(n.code,{children:"bgr8"})]}),"\n",(0,r.jsx)(n.h3,{id:"point-cloud-messages",children:"Point Cloud Messages"}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.code,{children:"sensor_msgs/PointCloud2"})," is a common data structure used to represent 3D point cloud data. Point clouds are collections of points in 3D space and are captured b y sensors such as 3D LIDARS or depth cameras. It consists of a collection of points in 3D space, each point having attributes such as its X, Y, and Z coordinates, intensity, color, or other relevant info. More info can be found ",(0,r.jsx)(n.a,{href:"https://docs.ros.org/en/melodic/api/sensor_msgs/html/msg/PointCloud2.html",children:"here"})]}),"\n",(0,r.jsx)(n.p,{children:"The most important fields are:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"height"})," and ",(0,r.jsx)(n.code,{children:"width"})," - represents the dimensions of the point cloud. Total number of points is ",(0,r.jsx)(n.strong,{children:"height * width"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"fields"})," - describes the individual data fields associated with each point, such as x, y, z coordinates, intensity, colors, etc. Specifies what attributes each point has."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"point_step"})," - represents the size of each point in bytes"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"row_step"})," - length of a row in bytes"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"data"})," - represents the raw binary data containing the point cloud and is 1D array. (Binary Format and size is ",(0,r.jsx)(n.strong,{children:"row_step x height"}),")"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"is_dense"})," - boolean indicating if the point if dense or sparse"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"A sample PointCloud2 message:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-txt",children:"- 0\n- 0\n- '...'\nis_dense: true\n---\nheader:\n  stamp:\n    sec: 414\n    nanosec: 253000000\n  frame_id: deepmind_robot1_camera_depth_optical_frame\nheight: 1\nwidth: 307200\nfields:\n- name: x\n  offset: 0\n  datatype: 7\n  count: 1\n- name: y\n  offset: 4\n  datatype: 7\n  count: 1\n- name: z\n  offset: 8\n  datatype: 7\n  count: 1\n- name: rgb\n  offset: 16\n  datatype: 7\n  count: 1\nis_bigendian: false\npoint_step: 32\nrow_step: 9830400\ndata:\n- 20\n- 159\n- 146\n- 191\n"})}),"\n",(0,r.jsx)(n.p,{children:"Given the following specifications:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"point_step"})," = 32"]}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.code,{children:"fields"})}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"x"})," - (offset = 0, datatype = 7)"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"y"})," - (offset = 4, datatype = 7)"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"z"})," - (offset = 8, datatype = 7)"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"rgb"})," - (offset - 16, datatype = 7)"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["Datatype = 7 denotes ",(0,r.jsx)(n.strong,{children:"float32"}),". The bytes 12 to 15 are unused as well as the bytes after 19 until the end of the 32-byte point representation. This repeats for each point in the point cloud data."]}),"\n",(0,r.jsx)("img",{src:d().src,width:"100%"}),"\n",(0,r.jsx)(n.h5,{id:"point-cloud-marker-written-by-the-construct",children:"Point Cloud Marker written by The Construct"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-py",children:'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import PointCloud2\nfrom visualization_msgs.msg import Marker, MarkerArray\nfrom geometry_msgs.msg import Point\nimport struct\nfrom typing import List\n\nclass PointCloudMarker(Node):\n\n    def __init__(self) -> None:\n        super().__init__(\'point_cloud_marker\')\n        self.subscription = self.create_subscription(\n            PointCloud2,\n            \'/deepmind_robot1_depth_sensor/points\',\n            self.point_cloud_callback,\n            10)\n        self.marker_publisher = self.create_publisher(MarkerArray, \'visualization_marker_array\', 10)\n\n    def point_cloud_callback(self, msg: PointCloud2) -> None:\n\n        # Extract points from the point cloud message uniformly\n        points = self.extract_points(msg)\n\n        # Create markers for the extracted points\n        marker_array = self.create_markers(msg, points)\n\n        # Publish the marker array\n        self.marker_publisher.publish(marker_array)\n\n    def extract_points(self, msg: PointCloud2) -> List[Point]:\n        """Extract points uniformly from the point cloud message"""\n        points = []\n        num_points = 50\n        step_size = len(msg.data) // num_points  # Divide the data into 50 equal parts\n\n        for i in range(0, len(msg.data), step_size):\n            if len(points) >= num_points:\n                break\n\n            index = i\n            x = self.read_float32(msg.data, index)\n            y = self.read_float32(msg.data, index + 4)\n            z = self.read_float32(msg.data, index + 8)\n            p = Point()\n            p.x, p.y, p.z = x, y, z\n            points.append(p)\n\n        return points\n\n    def create_markers(self, msg: PointCloud2, points: List[Point]) -> MarkerArray:\n        """Create markers for the extracted points"""\n        marker_array = MarkerArray()\n        for idx, point in enumerate(points):\n            marker = Marker()\n            marker.header = msg.header\n            marker.type = Marker.SPHERE\n            marker.action = Marker.ADD\n            marker.scale.x = 0.1\n            marker.scale.y = 0.1\n            marker.scale.z = 0.1\n            marker.color.r = 1.0\n            marker.color.a = 1.0\n            marker.pose.position = point\n            marker.id = idx\n            marker_array.markers.append(marker)\n\n        return marker_array\n\n    def read_float32(self, data: bytes, index: int) -> float:\n        """Helper function to read float32 from byte array"""\n        return struct.unpack_from(\'f\', data, index)[0]\n\ndef main(args=None) -> None:\n    rclpy.init(args=args)\n    point_cloud_marker = PointCloudMarker()\n    rclpy.spin(point_cloud_marker)\n    point_cloud_marker.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,r.jsx)("img",{src:h,width:"100%"}),"\n",(0,r.jsx)(n.h2,{id:"image-processing",children:"Image Processing"}),"\n",(0,r.jsx)(n.p,{children:"Image processing is used to improve the quality of an image or extract useful information from it. This part will use OpenCV and cv_bridge to for efficient conversion/integration between ROS2 messages and OpenCV images."}),"\n",(0,r.jsx)(n.h3,{id:"opencv",children:"OpenCV"}),"\n",(0,r.jsx)(n.p,{children:"If you don't know what OpenCV is, then you definitely need to read up on it. It's the premise for all computer vision things. It's an open source computer vision and machine learning software library that provides an infrastructure for computer vision applications. You can do various things such as image recognition, object detection, facial recognition, and video tracking."}),"\n",(0,r.jsx)(n.p,{children:"Here are some of the features OpenCV provides:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Filtering"})," - Offers filters such as Gaussian blur, median blur, and bilateral filter for noise reduction and image smoothing."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Feature Extraction"})," - Offers algorithms like SIFT, SURF, and ORB for feature extraction and keypoint detection which are crucial for tasks like object recognition and tracking."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Object Detection"})," - Has pretained models and algorithms such as Haar cascades, HOG descriptors, and deep learning based methods for object detection and classification."]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["OpenCV functions can be directly applied to the image data extracted from ",(0,r.jsx)(n.code,{children:"sensor_msgs/Image"})," which makes it easy to use in ROS2 projects."]}),"\n",(0,r.jsx)(n.h3,{id:"cv_bridge",children:"cv_bridge"}),"\n",(0,r.jsxs)(n.p,{children:["Before using OpenCV functions, image data needs to be converted from the ROS2 message format to a format compatible with OpenCV. This is done through the ",(0,r.jsx)(n.strong,{children:"cv_bridge"})," package, which converts between ROS2 image messages (sensor_msgs/Image) and OpenCV data structures (cv::mat). This allows developer to focus on the actual development instead of the conversion and integration of different tools."]}),"\n",(0,r.jsx)("img",{src:u().src,width:"100%"}),"\n",(0,r.jsx)(n.h3,{id:"color-spaces",children:"Color Spaces"}),"\n",(0,r.jsx)(n.p,{children:"A color space is a mathematical model that describes how colors can be represented as tuples of numbers, typically as coordinates or vectors in a multidimensional space. Different colors have different purposes and each have their tradeoffs depending on the problem at hand."}),"\n",(0,r.jsx)(n.p,{children:"Some important color spaces:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"RGB"})," (Red, Green, Blue)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"HSV"})," (Hue, Saturation, Value)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"HSL"})," (Hue, Saturation, Lightness)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"YUV/YCbCR"})," (Luma, Chroma)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"LAB"})," (CIELAB)"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Choosing the right color space can impact the success and efficiency of our image processing algorithms. As developers always say, choose the right tool for the job."}),"\n",(0,r.jsx)(n.h4,{id:"rgb-vs-hsv",children:"RGB vs HSV"}),"\n",(0,r.jsx)(n.p,{children:"RGB is one of the most common colors used in digital imaging and as the acronym suggests, it stands for (Red, Green, Blue). Each value ranges from 0 to 255 and represents the intensity of each color channel. Generally, RGB is great at representing colors on electronic displays and digital cameras."}),"\n",(0,r.jsx)(n.p,{children:"HSV is an alternative color space which uses (Hue, Saturation, and Value). Hue represents the type of color (red, green, blue) and is measured in degrees on a color wheel. Saturation determines the intensity or the purity of the color. Higher saturation values indicates more vivid colors. Value represents the brightness or intensity of the color. HSV is designed to more closely align with how humans perceive and describe colors."}),"\n",(0,r.jsx)(n.h4,{id:"advantages-of-hsv-over-rgb-for-image-processing",children:"Advantages of HSV over RGB for Image Processing"}),"\n",(0,r.jsx)(n.p,{children:"HSV is often preferred over RGB in color segmentation which is when we identify specific objects or regions of interests in an image. The HSV color space simplifies the process of defining color ranges and thresholds for segmentation. This is because thresholding based on hue and saturation help isolate specific colors or color ranges while minimizing the impact of variations in lighting conditions."}),"\n",(0,r.jsx)(n.p,{children:"HSV is also more robust to changes in lighting conditions compared to RGB. Decoupling color information from brightness (value) allows us to handle different variations in illumination without affecting color perception."}),"\n",(0,r.jsx)(n.h2,{id:"blob-tracking",children:"Blob Tracking"}),"\n",(0,r.jsx)(n.p,{children:"Blob tracking is a technique used to detect and track regions of interests (blob) in a sequence of images or frames in a video stream. A blob typically represents a group of connected pixels with similar properties, such as color, intensity, or texture."}),"\n",(0,r.jsx)(n.h3,{id:"blob-tracker-node",children:"Blob Tracker Node"}),"\n",(0,r.jsx)(n.p,{children:"The simulation goal is for our robot to detect orange blobs which is situated at the exit. This allows our robot to navigate towards the exit."}),"\n",(0,r.jsx)(n.p,{children:"We need to create two things:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"blob_point_pub.py"})," - Detect blob and publish the position"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"blob_tracker.py"})," - Guide the robot towards the exit door"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"We have to first isolate out the blob which requires filtering based on HSV color. The simulation provides us some fancy code that allows us to isolate the HSV."}),"\n",(0,r.jsx)("img",{src:g().src,width:"100%"}),"\n",(0,r.jsx)(n.p,{children:"We can then proceed to create our two necessary files."}),"\n",(0,r.jsxs)(o.A,{children:[(0,r.jsx)(a.A,{label:"blob_point_pub.py",value:"blob_point_pub.py",children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-py",children:'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom geometry_msgs.msg import Point\nimport cv2\nimport numpy as np\nfrom cv_bridge import CvBridge\nfrom typing import List, Tuple\n\nclass BlobPointPublisher(Node):\ndef **init**(self) -> None:\nsuper().**init**(\'blob_point_publisher\')\nself.subscription = self.create_subscription(\nImage,\n\'/deepmind_robot1/deepmind_robot1_camera/image_raw\',  \n self.image_callback, 10)\nself.publisher = self.create_publisher(Point, \'/point_blob\', 10)\nself.cv_bridge = CvBridge()\n\n    def image_callback(self, msg: Image) -> None:\n        try:\n            # Convert ROS Image message to OpenCV format\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding="bgr8")\n        except Exception as e:\n            self.get_logger().error("Error converting ROS Image to OpenCV format: {0}".format(e))\n            return\n\n        # Convert BGR image to HSV\n        hsv_image = cv2.cvtColor(cv_image, cv2.COLOR_BGR2HSV)\n\n        # Define lower and upper bounds for orange color in HSV\n        lower_orange = np.array([3, 0, 233])\n        upper_orange = np.array([9, 255, 255])\n\n        # Create a binary mask for orange color\n        mask = cv2.inRange(hsv_image, lower_orange, upper_orange)\n\n       # Draw coordinate frame\n        self.draw_frame(cv_image)\n\n        # Find contours in the mask\n        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        # Draw a red circle around the blob contour\n        for contour in contours:\n            self.draw_blob_contour_circle(cv_image, contour)\n\n        # If no contours are found, return\n        if not contours:\n            return\n\n        # Find the largest contour\n        blob_contour = max(contours, key=cv2.contourArea)\n\n        # Calculate centroid of the contour\n        moment = cv2.moments(blob_contour)\n        if moment["m00"] != 0:             # Area of the object\n            blob_x = float(moment["m10"] / moment["m00"])  # x-coordinate of centroid\n            blob_y = float(moment["m01"] / moment["m00"])  # y-coordinate of centroid\n\n            # x and y coordinates of the blob are computed relative to the center of the image\n            x, y = self.get_blob_relative_position(cv_image, blob_x, blob_y)\n\n            # Publish Blob\n            self.publish_blob(x, y)\n\n            # Draw detected blobs\n            self.draw_keypoints(cv_image, [(blob_x, blob_y)])\n\n        # Display the image (optional)\n        cv2.imshow("Image", cv_image)\n        cv2.waitKey(1)\n\n    def draw_frame(self,\n                image: np.ndarray,\n                dimension=0.1,      #- dimension relative to frame size\n                line=2              #- line\'s thickness\n        ) -> None:\n        """ Draw X Y coordinate frame at the center of the image"""\n\n        rows = image.shape[0]\n        cols = image.shape[1]\n        size = min([rows, cols])\n        center_x = int(cols/2.0)\n        center_y = int(rows/2.0)\n\n        line_length = int(size*dimension)\n\n        #-- X\n        image = cv2.line(image, (center_x, center_y), (center_x+line_length, center_y), (0,0,255), line)\n        #-- Y\n        image = cv2.line(image, (center_x, center_y), (center_x, center_y+line_length), (0,255,0), line)\n\n\n    def draw_blob_contour_circle(self, img: np.ndarray, contour: np.ndarray) -> None:\n        """ Draw a red circle around the detected blob """\n        # Get the bounding rectangle of the contour\n        x, y, w, h = cv2.boundingRect(contour)\n\n        # Calculate the center of the rectangle\n        center_x = x + w // 2\n        center_y = y + h // 2\n\n        # Calculate the radius of the circle based on the contour size\n        radius = max(w, h) // 2\n\n        # Draw a red circle around the center\n        cv2.circle(img, (center_x, center_y), radius, (0, 0, 255), 2)\n\n\n    def get_blob_relative_position(self, image: np.ndarray, x: float, y: float) -> Tuple[float, float]:\n        """ Get blob position relative to the coordinate frame placed at the center of the image"""\n        # The shape attribute of a NumPy array returns a tuple representing the dimensions of the array.\n        # For an image, the shape tuple consists of 3 elements: (height, width, channels).\n\n        rows = float(image.shape[0]) # height\n        cols = float(image.shape[1]) # width\n\n        #  Coordinates of the center of the image\n        center_x    = 0.5*cols\n        center_y    = 0.5*rows\n\n        # The x and y coordinates of the keypoint are computed relative to the center of the image\n        x = (x - center_x)/(center_x)\n        y = (y - center_y)/(center_y)\n\n        return x,y\n\n    def publish_blob(self, x: float, y: float) -> None:\n        """ Publish the blob position to /point_blob """\n        # Create a Point message\n        point_msg = Point()\n        point_msg.x = x\n        point_msg.y = y\n\n        # Publish the Point message\n        self.publisher.publish(point_msg)\n\n\n    def draw_keypoints(self, img: np.ndarray, keypoints: List[Tuple[float, float]]) -> None:\n        """ Draw the detected blob keypoints in red """\n        # Draw red circles at the detected blob keypoints\n        for kp in keypoints:\n            # Convert keypoints to integers\n            kp = (int(kp[0]), int(kp[1]))\n            cv2.circle(img, kp, 5, (0, 0, 255), -1)\n\ndef main(args=None) -> None:\nrclpy.init(args=args)\nblob_point_publisher = BlobPointPublisher()\nrclpy.spin(blob_point_publisher)\nblob_point_publisher.destroy_node()\nrclpy.shutdown()\n\nif **name** == \'**main**\':\nmain()\n\n'})})}),(0,r.jsx)(a.A,{label:"blob_tracker.py",value:"blob_tracker.py",children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-py",children:"import rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Point, Twist\n\nclass BlobTracker(Node):\n    def __init__(self) -> None:\n        super().__init__('blob_tracker')\n        self.subscription = self.create_subscription(\n            Point,\n            '/point_blob',\n            self.blob_callback,\n            10)\n        self.publisher = self.create_publisher(Twist, '/cmd_vel', 10)\n\n        self.angular_gain = 0.3\n        self.no_blob_detected = True\n        self.timer = self.create_timer(1.0, self.rotate_continuous)\n\n    def blob_callback(self, msg: Point) -> None:\n        # Extract blob coordinates\n        blob_x = msg.x\n        blob_y = msg.y\n\n        # Define linear velocity\n        linear_x = 0.3\n\n        if blob_x != 0.0 or blob_y != 0.0:\n            # Adjust angular velocity based on blob's position\n            angular_vel = -self.angular_gain * blob_x\n            # Clip angular velocity to [-1, 1] range\n            angular_vel = max(min(angular_vel, 1.0), -1.0)\n\n            # Adjust linear velocity based on blob's position\n            if blob_y < 0.8:\n                linear_vel = linear_x\n            else:\n                linear_vel = 0.0  # Stop the robot\n            self.no_blob_detected = False\n        else:\n            return\n\n        self.pub_velocities(linear_vel, angular_vel)\n\n    def rotate_continuous(self) -> None:\n        # Rotate continuously\n        if self.no_blob_detected:\n            twist_msg = Twist()\n            twist_msg.angular.z = 0.3  # Adjust angular velocity as needed\n            self.publisher.publish(twist_msg)\n            self.get_logger().warn(\"No blobs detected yet ... Rotating continuously\")\n\n    def pub_velocities(self, linear: float, angular: float) -> None:\n        # Create Twist message with linear and angular velocities\n        twist_msg = Twist()\n        twist_msg.linear.x = linear\n        twist_msg.angular.z = angular\n\n        # Publish the Twist message\n        self.publisher.publish(twist_msg)\n        self.get_logger().info(f\"Linear vel: {linear:.2f}, Angular vel: {angular:.2f}\")\n\n\ndef main(args=None) -> None:\n    rclpy.init(args=args)\n    blob_tracker = BlobTracker()\n    rclpy.spin(blob_tracker)\n    blob_tracker.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})})})]})]})}function y(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(v,{...e})}):v(e)}},9365:(e,n,t)=>{"use strict";t.d(n,{A:()=>o});t(6540);var s=t(4164);const r={tabItem:"tabItem_Ymn6"};var i=t(4848);function o(e){let{children:n,hidden:t,className:o}=e;return(0,i.jsx)("div",{role:"tabpanel",className:(0,s.A)(r.tabItem,o),hidden:t,children:n})}},1470:(e,n,t)=>{"use strict";t.d(n,{A:()=>j});var s=t(6540),r=t(4164),i=t(3104),o=t(6347),a=t(205),l=t(7485),c=t(1682),d=t(679);function h(e){return s.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,s.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function p(e){const{values:n,children:t}=e;return(0,s.useMemo)((()=>{const e=n??function(e){return h(e).map((e=>{let{props:{value:n,label:t,attributes:s,default:r}}=e;return{value:n,label:t,attributes:s,default:r}}))}(t);return function(e){const n=(0,c.XI)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,t])}function u(e){let{value:n,tabValues:t}=e;return t.some((e=>e.value===n))}function m(e){let{queryString:n=!1,groupId:t}=e;const r=(0,o.W6)(),i=function(e){let{queryString:n=!1,groupId:t}=e;if("string"==typeof n)return n;if(!1===n)return null;if(!0===n&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:n,groupId:t});return[(0,l.aZ)(i),(0,s.useCallback)((e=>{if(!i)return;const n=new URLSearchParams(r.location.search);n.set(i,e),r.replace({...r.location,search:n.toString()})}),[i,r])]}function g(e){const{defaultValue:n,queryString:t=!1,groupId:r}=e,i=p(e),[o,l]=(0,s.useState)((()=>function(e){let{defaultValue:n,tabValues:t}=e;if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(n){if(!u({value:n,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${n}" but none of its children has the corresponding value. Available values are: ${t.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return n}const s=t.find((e=>e.default))??t[0];if(!s)throw new Error("Unexpected error: 0 tabValues");return s.value}({defaultValue:n,tabValues:i}))),[c,h]=m({queryString:t,groupId:r}),[g,f]=function(e){let{groupId:n}=e;const t=function(e){return e?`docusaurus.tab.${e}`:null}(n),[r,i]=(0,d.Dv)(t);return[r,(0,s.useCallback)((e=>{t&&i.set(e)}),[t,i])]}({groupId:r}),b=(()=>{const e=c??g;return u({value:e,tabValues:i})?e:null})();(0,a.A)((()=>{b&&l(b)}),[b]);return{selectedValue:o,selectValue:(0,s.useCallback)((e=>{if(!u({value:e,tabValues:i}))throw new Error(`Can't select invalid tab value=${e}`);l(e),h(e),f(e)}),[h,f,i]),tabValues:i}}var f=t(2303);const b={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var x=t(4848);function _(e){let{className:n,block:t,selectedValue:s,selectValue:o,tabValues:a}=e;const l=[],{blockElementScrollPositionUntilNextRender:c}=(0,i.a_)(),d=e=>{const n=e.currentTarget,t=l.indexOf(n),r=a[t].value;r!==s&&(c(n),o(r))},h=e=>{let n=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":{const t=l.indexOf(e.currentTarget)+1;n=l[t]??l[0];break}case"ArrowLeft":{const t=l.indexOf(e.currentTarget)-1;n=l[t]??l[l.length-1];break}}n?.focus()};return(0,x.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,r.A)("tabs",{"tabs--block":t},n),children:a.map((e=>{let{value:n,label:t,attributes:i}=e;return(0,x.jsx)("li",{role:"tab",tabIndex:s===n?0:-1,"aria-selected":s===n,ref:e=>{l.push(e)},onKeyDown:h,onClick:d,...i,className:(0,r.A)("tabs__item",b.tabItem,i?.className,{"tabs__item--active":s===n}),children:t??n},n)}))})}function v(e){let{lazy:n,children:t,selectedValue:i}=e;const o=(Array.isArray(t)?t:[t]).filter(Boolean);if(n){const e=o.find((e=>e.props.value===i));return e?(0,s.cloneElement)(e,{className:(0,r.A)("margin-top--md",e.props.className)}):null}return(0,x.jsx)("div",{className:"margin-top--md",children:o.map(((e,n)=>(0,s.cloneElement)(e,{key:n,hidden:e.props.value!==i})))})}function y(e){const n=g(e);return(0,x.jsxs)("div",{className:(0,r.A)("tabs-container",b.tabList),children:[(0,x.jsx)(_,{...n,...e}),(0,x.jsx)(v,{...n,...e})]})}function j(e){const n=(0,f.A)();return(0,x.jsx)(y,{...e,children:h(e.children)},String(n))}},8453:(e,n,t)=>{"use strict";t.d(n,{R:()=>o,x:()=>a});var s=t(6540);const r={},i=s.createContext(r);function o(e){const n=s.useContext(i);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);